# ğŸ§  Multimodal Retrieval Using Unstructured Data

## ğŸ“Œ Overview
This project demonstrates a **Multimodal Retrieval-Augmented Generation (RAG) pipeline** for **PDF documents** containing **text, tables, and images**. It uses **Unstructured.io** to parse documents, **LangChain** to manage retrieval and generation, and integrates **Chroma** as a vector database. The pipeline supports advanced querying over heterogeneous data types to create context-aware responses.

## âœ¨ Features
- ğŸ“„ **PDF Partitioning** â€“ Extracts text, tables, and images from PDFs using `unstructured.partition.pdf`.
- ğŸ–¼ **Multimodal Support** â€“ Separates and processes text, table, and image elements.
- ğŸ§  **Vector Store Integration** â€“ Stores embeddings in **Chroma** for efficient retrieval.
- âš¡ **LangChain Components** â€“ Uses `MultiVectorRetriever` and HuggingFace embeddings.
- ğŸ¤– **LLM Querying** â€“ Employs **Groq** or **OpenAI** models via LangChain for generating answers.
- ğŸ”— **Flexible Prompting** â€“ Utilizes `ChatPromptTemplate` and `StrOutputParser`.
- ğŸ§° **Interactive Visualization** â€“ Displays extracted images directly in notebooks.

## ğŸ§­ Workflow
```
PDF Document 
    â”‚
    â–¼
Partition PDF â†’ Extract [Text, Tables, Images]
    â”‚
    â–¼
Generate Embeddings (HuggingFace)
    â”‚
    â–¼
Store in Chroma Vector DB
    â”‚
    â–¼
Retrieve Relevant Chunks (MultiVectorRetriever)
    â”‚
    â–¼
LLM (ChatGroq / OpenAI) â†’ Context-Aware Answer
```

## ğŸš€ Getting Started

### 1ï¸âƒ£ Prerequisites
- Python 3.9+
- Jupyter Notebook or Google Colab
- API Keys for **Groq** or **OpenAI**

### 2ï¸âƒ£ Installation
Install required dependencies:
```bash
pip install unstructured langchain langchain-groq langchain-huggingface chromadb openai
```

### 3ï¸âƒ£ Usage
1. Open the notebook:
   ```bash
   jupyter notebook Multimodal_Retrieval_Unstructured.ipynb
   ```
2. Upload a PDF file in the notebook or Google Colab.
3. Set your API key:
   ```python
   from google.colab import userdata
   os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
   ```
4. Run the notebook cells to:
   - Partition the PDF into text, tables, and images.
   - Embed and store the elements in Chroma.
   - Query the LLM for context-rich answers.

## ğŸ“‚ Directory Structure
```
â”œâ”€â”€ Multimodal_Retrieval_Unstructured.ipynb   # Main pipeline
â”œâ”€â”€ requirements.txt                          # (Optional) List of dependencies
â””â”€â”€ data/                                     # (Optional) PDF files and samples
```

## ğŸ§° Technologies Used
- **[Unstructured.io](https://unstructured.io/)** â€“ PDF parsing and element extraction
- **[LangChain](https://www.langchain.com/)** â€“ Framework for RAG pipeline
- **[Chroma](https://www.trychroma.com/)** â€“ Vector database for embeddings
- **[Hugging Face Transformers](https://huggingface.co/)** â€“ Embedding models
- **[Groq LLM](https://groq.com/)** or **[OpenAI](https://openai.com/)** â€“ Large language models

## ğŸ§ª Example Query
```python
query = "Summarize the key points from this PDF."
result = retriever.get_relevant_documents(query)
print(result)
```

## ğŸ§­ Roadmap
- [ ] Integrate **table-to-text** parsing for richer table insights.
- [ ] Add **image captioning** for image-based retrieval.
- [ ] Deploy as a **FastAPI** or **Streamlit** web app.

## ğŸ™Œ Acknowledgements
Special thanks to **LangChain**, **Unstructured**, **Chroma**, and **Hugging Face** for their powerful open-source tools.
